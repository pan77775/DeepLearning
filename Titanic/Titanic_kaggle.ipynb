{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Common Model Algorithms\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#Common Model Helpers\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn import model_selection\n",
    "\n",
    "#Visualization\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import data from file: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html\n",
    "data_org = pd.read_csv('train.csv')\n",
    "data_test_org = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data info\n",
    "print(data_org.head())\n",
    "print(data_org.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to play with our data we'll create a copy\n",
    "data_train = data_org.copy(deep = True)\n",
    "data_test = data_test_org.copy(deep = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make list to clean both datasets at once\n",
    "data_cleaner = [data_train, data_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean data\n",
    "for dataset in data_cleaner:\n",
    "    #missing value in Fare and Embarked (Age later)\n",
    "    #complete Embarked with mode\n",
    "    dataset['Embarked'].fillna(dataset['Embarked'].mode()[0], inplace = True)\n",
    "    #complete missing Fare with median\n",
    "    dataset['Fare'].fillna(dataset['Fare'].median(), inplace = True)\n",
    "    \n",
    "    #new feature Title from name\n",
    "    dataset['Title'] = dataset['Name'].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]\n",
    "    stat_min = 10 #while small is arbitrary, we'll use the common minimum in statistics: http://nicholasjjackson.com/2012/03/08/sample-size-is-10-a-magic-number/\n",
    "    title_names = (dataset['Title'].value_counts() < stat_min) #this will create a true false series with title name as index\n",
    "    dataset['Title'] = dataset['Title'].apply(lambda x: 'Misc' if title_names.loc[x] == True else x)\n",
    "    \n",
    "    #new feature Family size\n",
    "    dataset['FamilySize'] = dataset ['SibSp'] + dataset['Parch'] + 1\n",
    "    \n",
    "    #new feature IsAlone\n",
    "    dataset['IsAlone'] = 1 #initialize to yes/1 is alone\n",
    "    dataset['IsAlone'].loc[dataset['FamilySize'] > 1] = 0 # now update to no/0 if family size is greater than 1\n",
    "       \n",
    "    # create dummy variables for Pclass column, & drop 3rd class as it has the lowest average of survived passengers\n",
    "    dummy_pclass = pd.get_dummies(dataset['Pclass'])\n",
    "    dummy_pclass.columns = ['Class_1','Class_2','Class_3']\n",
    "    dummy_pclass.drop(['Class_3'], axis=1, inplace=True)\n",
    "    dataset.drop(['Pclass'], axis=1, inplace=True)\n",
    "    dataset['Class_1'] = dummy_pclass['Class_1']\n",
    "    dataset['Class_2'] = dummy_pclass['Class_2']\n",
    "    \n",
    "    #drop feature\n",
    "    drop_feature=['PassengerId','Name','Ticket','Cabin']\n",
    "    dataset.drop(drop_feature, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define x and y variables for dummy features original\n",
    "train_dummy = pd.get_dummies(data_train)\n",
    "test_dummy = pd.get_dummies(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#complete Age with RF\n",
    "train_data_age = train_dummy['Age']>0\n",
    "train_age_x = train_dummy.drop(['Age','Survived'], axis=1, inplace=False).loc[train_data_age]\n",
    "train_age_y = train_dummy['Age'].loc[train_data_age]\n",
    "train_missing_age_x = train_dummy.drop(['Age','Survived'], axis=1, inplace=False).loc[train_data_age == False]\n",
    "test_data_age = test_dummy['Age']>0\n",
    "test_missing_age_x = test_dummy.drop(['Age'], axis=1, inplace=False).loc[test_data_age == False]\n",
    "\n",
    "# Fitting RandomForest to the dataset\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "regressor = RandomForestRegressor(random_state = 10, warm_start = True, \n",
    "                                  n_estimators = 26,\n",
    "                                  max_depth = 6, \n",
    "                                  max_features = 'sqrt')\n",
    "regressor.fit(train_age_x, train_age_y)\n",
    "train_dummy['Age'][train_data_age == False] = regressor.predict(train_missing_age_x)\n",
    "test_dummy['Age'][test_data_age == False] = regressor.predict(test_missing_age_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if age under 12 ,set sex to child(not male or female) \n",
    "train_dummy['Sex_female'][train_dummy['Age'] <= 12] = 0\n",
    "train_dummy['Sex_male'][train_dummy['Age'] <= 12] = 0\n",
    "test_dummy['Sex_female'][test_dummy['Age'] <= 12] = 0\n",
    "test_dummy['Sex_male'][test_dummy['Age'] <= 12] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data to train and cv\n",
    "train_x, cv_x, train_y, cv_y = model_selection.train_test_split(train_dummy.drop(['Survived'], axis=1, inplace=False), train_dummy['Survived'], test_size = 0.2, random_state = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_X = StandardScaler()\n",
    "train_x = sc_X.fit_transform(train_x)\n",
    "cv_x = sc_X.transform(cv_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting SVM to the Training set\n",
    "from sklearn.svm import SVC\n",
    "classifier = SVC(C=1, kernel = 'rbf' , random_state = 0)\n",
    "classifier.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(cv_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "acc = accuracy_score(cv_y, y_pred, normalize=True, sample_weight=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_x Feature scaling\n",
    "test_x = sc_X.fit_transform(test_dummy)\n",
    "#predict test data\n",
    "y_pred_test = classifier.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output \n",
    "b = np.c_[data_test_org['PassengerId'],y_pred_test]\n",
    "ans = pd.DataFrame(b)\n",
    "ans.columns = ['PassengerId','Survived']\n",
    "ans.to_csv('titanic_svm_01.csv', index = False, header = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
